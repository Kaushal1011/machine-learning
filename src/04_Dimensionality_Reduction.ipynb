{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Approaches for Dimensionality Reduction\n",
    "\n",
    "- ___Projection___\n",
    "- ___Manifold___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection\n",
    "\n",
    "In most real-world problems, training instances are not spread out uniformly across\n",
    "all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances actually lie within\n",
    "(or close to) a much lower-dimensional subspace of the high-dimensional space.\n",
    "\n",
    "__Explaining beyond this more clearly requires more reading of this topic.__\n",
    "\n",
    "__i hope the images help a bit.__\n",
    "\n",
    "we took a dataset and found that this can be squashed into a hyperplane with no change in interpretation of data.\n",
    "\n",
    "![Projection1](img/projection1.PNG)\n",
    "\n",
    "Projected Dataset Plot\n",
    "\n",
    "![Projection2](img/projection2.PNG)\n",
    "\n",
    "However this doesnt always work as we see in this following example\n",
    "\n",
    "![Projection3](img/projection3.PNG)\n",
    "\n",
    "![Projection4](img/projection4.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning\n",
    "\n",
    "The Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D\n",
    "shape that can be bent and twisted in a higher-dimensional space. More generally, a\n",
    "d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\n",
    "resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\n",
    "locally resembles a 2D plane, but it is rolled in the third dimension.\n",
    "Many dimensionality reduction algorithms work by modeling the manifold on which\n",
    "the training instances lie; this is called Manifold Learning. It relies on the manifold\n",
    "assumption, also called the manifold hypothesis, which holds that most real-world\n",
    "high-dimensional datasets lie close to a much lower-dimensional manifold. This\n",
    "assumption is very often empirically observed.\n",
    "\n",
    "__\"The manifold assumption is often accompanied by another implicit assumption: that\n",
    "the task at hand (e.g., classification or regression) will be simpler if expressed in the\n",
    "lower-dimensional space of the manifold. For example, in the top row of Figure 8-6\n",
    "the Swiss roll is split into two classes: in the 3D space (on the left), the decision\n",
    "boundary would be fairly complex, but in the 2D unrolled manifold space (on the\n",
    "right), the decision boundary is a simple straight line.\n",
    "However, this assumption does not always hold. For example, in the bottom row of\n",
    "Figure 8-6, the decision boundary is located at x1\n",
    " = 5. This decision boundary looks\n",
    "very simple in the original 3D space (a vertical plane), but it looks more complex in\n",
    "the unrolled manifold (a collection of four independent line segments).\n",
    "In short, if you reduce the dimensionality of your training set before training a\n",
    "model, it will definitely speed up training, but it may not always lead to a better or\n",
    "simpler solution; it all depends on the dataset.\n",
    "Hopefully you now have a good sense of what the curse of dimensionality is and how\n",
    "dimensionality reduction algorithms can fight it, especially when the manifold\n",
    "assumption holds. The rest of this chapter will go through some of the most popular\n",
    "algorithms.\"__\n",
    "\n",
    "___See Figure for a better idea___\n",
    "\n",
    "![manifold](img/mani1.PNG)\n",
    "\n",
    "___As of now even I have hard time understanding this and further reading on this will help in getting a more clearer ides___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Principal Component Analysis\n",
    "\n",
    "First it identifies the hyperplane that lies closest to the data, and then\n",
    "it projects the data onto it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE - Locally Linear Embedding\n",
    "\n",
    " It is a Manifold Learning technique that does not rely\n",
    "on projections like the previous algorithms. In a nutshell, LLE works by first measurâ€\n",
    "ing how each training instance linearly relates to its closest neighbors (c.n.), and then\n",
    "looking for a low-dimensional representation of the training set where these local\n",
    "relationships are best preserved (more details shortly). This makes it particularly\n",
    "good at unrolling twisted manifolds, especially when there is not too much noise.\n",
    "\n",
    "# Incomplete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
