{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents \n",
    "\n",
    "- Ensemble Methods\n",
    "    - [Voting Classifier](#votclas)\n",
    "    - [Bagging vs Pasting](#bvp)\n",
    "    - [Random Patches and Random Subspaces](#rprs)\n",
    "    - [Random Forests](#rf)\n",
    "    - [Feature Importance](#fi)\n",
    "    - [Boosting](#boost)\n",
    "        - [AdaBoost](#ada)\n",
    "        - [Gradient Boosting](#grad)\n",
    "    - [Stacking](#stack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"votclas\"></a>\n",
    "## Voting Classifier\n",
    "\n",
    "If we have multiple classifiers each acheiving some accuracy and we want to create an even better classifier we use the voting classifier to predict based on predictions of our previously trained classifiers.\n",
    "I hope this image explains this in a better way.\n",
    "\n",
    "![Ensemble](img/Ensemble1.PNG)\n",
    "\n",
    "![Ensemble](img/Ensemble2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Hard Voting Classifier___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ensemble methods work best when the predictors are as independ‐\n",
    "ent from one another as possible. One way to get diverse classifiers\n",
    "is to train them using very different algorithms. This increases the\n",
    "chance that they will make very different types of errors, improving\n",
    "the ensemble’s accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Example\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    " estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    " voting='hard'\n",
    " )\n",
    "voting_clf.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"bvp\"></a>\n",
    "## Bagging vs Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use the same training algorithm for every\n",
    "predictor, but to train them on different random subsets of the training set.\n",
    "\n",
    "__When sampling is performed with replacement, this method is called bagging\n",
    " (short for\n",
    "bootstrap aggregating\n",
    ").__ \n",
    "\n",
    "__When sampling is performed without replacement, it is called\n",
    "pasting.__\n",
    "\n",
    "In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be\n",
    "sampled several times for the same predictor. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Example for bagging\n",
    "\n",
    "to use pasting just set bootstrap = False\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(), n_estimators=500,\n",
    " max_samples=100, bootstrap=True, n_jobs=-1\n",
    " )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The BaggingClassifier automatically performs soft voting\n",
    "instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a predict_proba() method), which is the case\n",
    "with Decision Trees classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ensemble3](img/Ensemble3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use Out-Of-Bag Evaluation for classification for BaggingClassifier.\n",
    "\n",
    "## Out Of Bag Evolution\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor,\n",
    "while others may not be sampled at all. By default a BaggingClassifier samples m\n",
    "training instances with replacement (bootstrap=True), where m is the size of the\n",
    "training set. This means that only about 63% of the training instances are sampled on\n",
    "average for each predictor.6\n",
    " The remaining 37% of the training instances that are not\n",
    "sampled are called out-of-bag (oob) instances. Note that they are not the same 37%\n",
    "for all predictors.\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on\n",
    "these instances, without the need for a separate validation set. You can evaluate the\n",
    "ensemble itself by averaging out the oob evaluations of each predictor.\n",
    "\n",
    "\n",
    "```python \n",
    "bag_clf.oob_score_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rprs\"></a>\n",
    "## Random Patches and Random Subspaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can sample features as well.Thus, training each predictor on a random subset of the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaggingClassifier class supports sampling the features as well. This is con‐\n",
    "trolled by two hyperparameters: max_features and bootstrap_features. They work\n",
    "the same way as max_samples and bootstrap, but for feature sampling instead of\n",
    "instance sampling. Thus, each predictor will be trained on a random subset of the\n",
    "input features.\n",
    "This is particularly useful when you are dealing with high-dimensional inputs (such\n",
    "as images). Sampling both training instances and features is called the __Random\n",
    "Patches method__.\n",
    " Keeping all training instances (i.e., bootstrap=False and max_sam\n",
    "ples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea\n",
    "tures smaller than 1.0) is called the __Random Subspaces method__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rf\"></a>\n",
    "## Random Forest\n",
    "\n",
    "Random Forest is an ensemble of Decision Trees, generally\n",
    "trained via the bagging method (or sometimes pasting), typically with max_samples\n",
    "set to the size of the training set. Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier\n",
    "class, which is more convenient and optimized for Decision Trees (similarly, there is\n",
    "a RandomForestRegressor class for regression tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read About ExtraTreesClassifier\n",
    "> It is hard to tell in advance whether a RandomForestClassifier\n",
    "will perform better or worse than an ExtraTreesClassifier. Generally, the only way to know is to try both and compare them using\n",
    "cross-validation (and tuning the hyperparameters using grid\n",
    "search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fi\"></a>\n",
    "# Feature Importance\n",
    "In a single Decision Tree, important features are likely to appear closer to the root of the tree, while unimportant features will often appear closer to\n",
    "the leaves (or not at all). It is therefore possible to get an estimate of a feature’s importance by computing the average depth at which it appears across all trees in the forest.\n",
    "Scikit-Learn computes this automatically for every feature after training. You can\n",
    "access the result using the feature_importances_ variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> iris = load_iris()\n",
    ">>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    ">>> rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    ">>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    ">>> print(name, score)\n",
    "sepal length (cm) 0.112492250999\n",
    "sepal width (cm) 0.0231192882825\n",
    "petal length (cm) 0.441030464364\n",
    "petal width (cm) 0.423357996355\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Random Forests are very handy to get a quick understanding of what features\n",
    "actually matter, in particular if you need to perform feature selection.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"boost\"></a>\n",
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Boosting (originally called hypothesis boosting) refers to any Ensemble method that\n",
    "can combine several weak learners into a strong learner.__\n",
    "Most Popular Boosting methods are :\n",
    "- AdaBoost(Adaptive Boosting)\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ada\"></a>\n",
    "### AdaBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an AdaBoost classifier, a first base classifier (such as a Decision\n",
    "Tree) is trained and used to make predictions on the training set. The relative weight\n",
    "of misclassified training instances is then increased. A second classifier is trained\n",
    "using the updated weights and again it makes predictions on the training set, weights\n",
    "are updated, and so on.\n",
    "\n",
    "![AdaBoost](img/AdaBoost.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is one important drawback to this sequential learning techni‐\n",
    "que: it cannot be parallelized (or only partially), since each predic‐\n",
    "tor can only be trained after the previous predictor has been\n",
    "trained and evaluated. As a result, it does not scale as well as bag‐\n",
    "ging or pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical Theory related to AdaBoost could be added here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn actually uses a multiclass version of AdaBoost called SAMME (which\n",
    "stands for Stagewise Additive Modeling using a Multiclass Exponential loss function).\n",
    "When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\n",
    "predictors can estimate class probabilities (i.e., if they have a predict_proba()\n",
    "method), Scikit-Learn can use a variant of SAMME called SAMME.R (the R stands\n",
    "for “Real”), which relies on class probabilities rather than predictions and generally\n",
    "performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    " DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    " algorithm=\"SAMME.R\", learning_rate=0.5\n",
    " )\n",
    "ada_clf.fit(X_train, y_train)\n",
    "```\n",
    "> If your AdaBoost ensemble is overfitting the training set, you can\n",
    "try reducing the number of estimators or more strongly regulariz‐\n",
    "ing the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"grad\"></a>\n",
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like AdaBoost,\n",
    "Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
    "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
    "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
    "errors made by the previous predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Very simple example to understand how Gradient Boosting works___\n",
    "\n",
    "Let’s go through a simple regression example using Decision Trees as the base predic‐\n",
    "tors (of course Gradient Boosting also works great with regression tasks). This is\n",
    "called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let’s\n",
    "fit a DecisionTreeRegressor to the training set (for example, a noisy quadratic train‐\n",
    "ing set):\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "```\n",
    "Now train a second DecisionTreeRegressor on the residual errors made by the first\n",
    "predictor:\n",
    "\n",
    "```python\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)\n",
    "```\n",
    "Then we train a third regressor on the residual errors made by the second predictor:\n",
    "```python\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)\n",
    "```\n",
    "Now we have an ensemble containing three trees. It can make predictions on a new\n",
    "instance simply by adding up the predictions of all the trees:\n",
    "\n",
    "```python\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe\n",
    "gressor class. Much like the RandomForestRegressor class, it has hyperparameters to\n",
    "control the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on),\n",
    "as well as hyperparameters to control the ensemble training, such as the number of\n",
    "trees (n_estimators).\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The learning_rate hyperparameter scales the contribution of each tree. If you set it\n",
    "to a low value, such as 0.1, you will need more trees in the ensemble to fit the train‐\n",
    "ing set, but the predictions will usually generalize better. This is a regularization tech‐\n",
    "nique called shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal number of trees, you can use early stopping. A simple way to implement this is to use the staged_predict() method: it\n",
    "returns an iterator over the predictions made by the ensemble at each stage of training (with one tree, two trees, etc.). The following code trains a GBRT ensemble with\n",
    "120 trees, then measures the validation error at each stage of training to find the optimal number of trees, and finally trains another GBRT ensemble using the optimal\n",
    "number of trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    " for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best way of early stopping\n",
    "```python\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The GradientBoostingRegressor class also supports a subsample hyperparameter,\n",
    "which specifies the fraction of training instances to be used for training each tree. For\n",
    "example, if subsample=0.25, then each tree is trained on 25% of the training instan‐\n",
    "ces, selected randomly. As you can probably guess by now, this trades a higher bias\n",
    "for a lower variance. It also speeds up training considerably. This technique is called\n",
    "Stochastic Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"stack\"></a>\n",
    "## Stacking\n",
    "\n",
    "Stacked generalisation:It is based on a simple idea: instead of using trivial functions\n",
    "(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\n",
    "why don’t we train a model to perform this aggregation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacking1](img/stacking1.PNG)\n",
    "\n",
    "Split Training Set into 2 and train layer 1 based on subset 1\n",
    "\n",
    "![Stacking2](img/stacking2.PNG)\n",
    "\n",
    "Train layer 2 based on predictions by layer 1 based on subset 2\n",
    "\n",
    "![Stacking3](img/stacking3.PNG)\n",
    "\n",
    "Final model\n",
    "\n",
    "![Stacking4](img/stacking4.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
